{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# %env CUDA_VISIBLE_DEVICES=0"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b47f1f5f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c30b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "\n",
    "from atlas.envs.xminigrid.mutators.common import Mutations\n",
    "from atlas.ued.buffer import BufferManager\n",
    "from atlas.utils.checkpointing import load_runner_state\n",
    "from atlas.utils.logging import download_checkpoints\n",
    "\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94096be",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32847a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_CAPACITY = 50000\n",
    "STALENESS_COEFF = 0.1\n",
    "NOTEBOOK_PATH = Path(os.getcwd()).parent\n",
    "PAD_VAL = -1   # sentinel for \"empty\" slots\n",
    "MUTATION_PADDING_ID = max(Mutations) + 10\n",
    "\n",
    "WANDB_ENTITY = None  # TODO: replace with your W&B entity and projects\n",
    "WANDB_PROJECT = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbd025c",
   "metadata": {},
   "source": [
    "# Download Checkpoints and Dump Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72561d20",
   "metadata": {},
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008281a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNS = {  # TODO: replace with W&B identifiers\n",
    "    \"accel_full-i\": [None] * 5,\n",
    "    \"accel_full-c\": [None] * 5,\n",
    "    \"accel_scratch-i\": [None] * 5,\n",
    "    \"accel_scratch-c\": [None] * 5,\n",
    "}\n",
    "\n",
    "# NOTE WE SKIP THE FIRST 100 STEPS BECAUSE THEY ARE NOT RELEVANT\n",
    "STEPS = list(range(110, 2000, 100)) + [2000]\n",
    "SEQUENTIAL = True\n",
    "\n",
    "def download_all_checkpoints(runs, steps, sequential=True):\n",
    "    for experiment in runs:\n",
    "        for run_id in runs[experiment]:\n",
    "            run_path = os.path.join(NOTEBOOK_PATH, \"ckpts\", 'sequential' if sequential else 'non_sequential', experiment, run_id)\n",
    "            if not Path(run_path).exists(): # Only download if the folder doesn't exist\n",
    "                download_checkpoints(WANDB_ENTITY, WANDB_PROJECT, run_id, steps, run_path)\n",
    "\n",
    "download_all_checkpoints(RUNS, STEPS, SEQUENTIAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5235694b",
   "metadata": {},
   "source": [
    "## Dump Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b3de82",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdb8917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_row_probabilities(row, row_probability, unique_mutations, pad_value=PAD_VAL):\n",
    "    \"\"\"\n",
    "    Returns a vector of length len(unique_mutations) whose i‑th entry is:\n",
    "        (# occurrences of unique_mutations[i] in `row`) * row_probability / (# valid entries)\n",
    "\n",
    "    Now duplicates (e.g. 2, 5 in your example) receive proportionally more weight.\n",
    "    \"\"\"\n",
    "    # mask out the padding\n",
    "    valid_mask = row != pad_value                   # (K,)\n",
    "\n",
    "    # broadcast comparison: (K,1) vs (1,M) → (K,M) bool\n",
    "    match = (row[:, None] == unique_mutations[None, :]) & valid_mask[:, None]\n",
    "\n",
    "    # count how many times each mutation appears in this row\n",
    "    counts = jnp.sum(match, axis=0)                 # (M,) int\n",
    "\n",
    "    # total real entries (counting duplicates)\n",
    "    n_real = jnp.sum(counts)                        # scalar\n",
    "\n",
    "    # probability per single occurrence\n",
    "    per_occurrence_mass = jnp.where(n_real > 0,\n",
    "                                    row_probability / n_real,\n",
    "                                    0.0)\n",
    "\n",
    "    # each mutation's probability = occurrences * per‑occurrence mass\n",
    "    return counts * per_occurrence_mass\n",
    "\n",
    "def get_mutation_probs_from_checkpoint(run_path, step, include_no_mutation=False):\n",
    "    \"\"\"\n",
    "    Returns mutation probabilities from a saved checkpoint.\n",
    "    \n",
    "    Returns:\n",
    "        unique_mutations: Array of unique mutation IDs (including pad_mutation_id for -1s)\n",
    "        mutation_prob_mass: Array of probability masses for each mutation\n",
    "    \"\"\"\n",
    "    runner_state = load_runner_state(\n",
    "        # resolve absolute path\n",
    "        path=Path(run_path).resolve(),\n",
    "        target=None,\n",
    "        step=step,\n",
    "    )\n",
    "    \n",
    "    buffer_manager = BufferManager(capacity=BUFFER_CAPACITY, staleness_coeff=STALENESS_COEFF)\n",
    "    probs = buffer_manager.problem_weights(runner_state['buffer'])\n",
    "    mutation_ids = runner_state['buffer']['extra']['mutation_ids']\n",
    "    \n",
    "    # Get unique mutations (excluding -1 padding)\n",
    "    unique_mutations = jnp.unique(mutation_ids[mutation_ids != -1])\n",
    "    \n",
    "    # Map -1 to the next available mutation ID after the max enum value\n",
    "    \n",
    "    # print(\"Padding ID is\", MUTATION_PADDING_ID)\n",
    "    unique_mutations = jnp.concatenate([unique_mutations, jnp.array([MUTATION_PADDING_ID])])\n",
    "\n",
    "    if include_no_mutation:\n",
    "        mutation_ids = jnp.where(mutation_ids == -1, MUTATION_PADDING_ID, mutation_ids)\n",
    "    else:\n",
    "        # Set probs to 0 for rows which are all -1 -> those which sum to -10, and then renormalize the other probs\n",
    "        no_mutations_mask = jnp.sum(mutation_ids, axis=-1) == -10\n",
    "        probs = probs.at[no_mutations_mask].set(0)\n",
    "        probs = probs / probs.sum()\n",
    "    \n",
    "    # vmap over the rows\n",
    "    get_probs_batched = jax.vmap(\n",
    "        get_row_probabilities,\n",
    "        in_axes=(0, 0, None)        # 1st arg row → axis 0, 2nd arg prob → axis 0, 3rd constant\n",
    "    )\n",
    "    all_row_probs = get_probs_batched(mutation_ids, probs, unique_mutations)\n",
    "    \n",
    "    # Sum across all rows to get total probability mass per mutation\n",
    "    mutation_prob_mass = all_row_probs.sum(axis=0)\n",
    "    \n",
    "    return unique_mutations, mutation_prob_mass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2662d8f1",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5592be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def mutation_to_str(mutation_id: Mutations) -> str:\n",
    "    match mutation_id:\n",
    "        case Mutations.ADD_NON_DOOR_OBJ:\n",
    "            string = \"AddObject\"\n",
    "        case Mutations.RM_NON_DOOR_OBJ:\n",
    "            string = \"RemoveObject\"\n",
    "        case Mutations.MOVE_NON_DOOR_OBJ:\n",
    "            string = \"MoveObject\"\n",
    "        case Mutations.MOVE_AGENT:\n",
    "            string = \"MoveAgent\"\n",
    "        case Mutations.REPLACE_DOOR:\n",
    "            string = \"ReplaceDoor\"\n",
    "        case Mutations.REPLACE_NON_DOOR:\n",
    "            string = \"ReplaceNonDoor\"\n",
    "        case Mutations.ADD_ROOMS:\n",
    "            string = \"AddRooms\"\n",
    "        case Mutations.RM_ROOMS:\n",
    "            string = \"RemoveRooms\"\n",
    "        case Mutations.SWITCH_PROP:\n",
    "            string = \"SwitchProposition\"\n",
    "        case Mutations.ADD_TRANSITION:\n",
    "            string = \"AddState\"\n",
    "        case Mutations.RM_TRANSITION:\n",
    "            string = \"RemoveState\"\n",
    "        case Mutations.HINDSIGHT_LVL_ONLY:\n",
    "            string = \"hindsight_lvl\"\n",
    "        case Mutations.HINDSIGHT_PRED:\n",
    "            string = \"ExtractPreceding\"\n",
    "        case Mutations.HINDSIGHT_SUCC:\n",
    "            string = \"ExtractSucceeding\"\n",
    "    return rf\"\\textsc{{{string}}}\"\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "import itertools\n",
    "\n",
    "_suffix_re = re.compile(r\"(-[a-zA-Z0-9_]+)$\")      # strip final \"-…\"\n",
    "def base_name(raw: str) -> str:\n",
    "    return _suffix_re.sub(\"\", raw).lower()\n",
    "\n",
    "# ── 1. global style ────────────────────────────────────────────────────────────\n",
    "BIG, BIGGER = \"x-large\", \"xx-large\"\n",
    "\n",
    "set2                    = sns.color_palette(\"Set2\", 8)   # plenty of spares\n",
    "base_to_pretty          = {\n",
    "    'dr':             \"DR\",\n",
    "    \"plr\":            r\"PLR$^{\\bot}$\",\n",
    "    \"accel_full\":     \"ACCEL\",\n",
    "    \"accel_scratch\":  \"ACCEL-0\",\n",
    "}\n",
    "base_to_color           = {\n",
    "    \"plr\":            set2[0],\n",
    "    \"accel_full\":     set2[1],\n",
    "    \"accel_scratch\":  set2[2],  \n",
    "    \"dr\":             set2[3],\n",
    "}\n",
    "spares                  = itertools.cycle(set2[4:])      # nice, unused colours\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fb84a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Use the new function\n",
    "run_path = NOTEBOOK_PATH / 'ckpts' / 'sequential' / 'accel_full-i' / RUNS[\"accel_full-i\"][0]\n",
    "step = 1900\n",
    "unique_mutations, mutation_prob_mass = get_mutation_probs_from_checkpoint(run_path, step, include_no_mutation=False)\n",
    "\n",
    "print(unique_mutations)\n",
    "print(mutation_prob_mass)\n",
    "print(f\"Total probability mass: {mutation_prob_mass.sum()}\")\n",
    "\n",
    "for mutation_id, prob_mass in zip(unique_mutations.tolist(), mutation_prob_mass.tolist()):\n",
    "    print(f\"Mutation {mutation_to_str(mutation_id) if mutation_id != MUTATION_PADDING_ID else 'None'}: {prob_mass}\")\n",
    "\n",
    "mutation_labels = [mutation_to_str(mutation_id) if mutation_id != MUTATION_PADDING_ID else 'None' for mutation_id in unique_mutations.tolist()]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=mutation_labels, y=mutation_prob_mass, palette=\"crest\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('Mutation Type')\n",
    "plt.ylabel('Probability Mass')\n",
    "plt.title('Mutation Probability Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28d27cb",
   "metadata": {},
   "source": [
    "### Dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db9f09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Check if cached data exists\n",
    "cache_file = NOTEBOOK_PATH / \"mutation_data_cache.pkl\"\n",
    "\n",
    "if cache_file.exists():\n",
    "    print(\"Loading cached mutation data...\")\n",
    "    with open(cache_file, 'rb') as f:\n",
    "        run_data = pickle.load(f)\n",
    "    \n",
    "    # Verify the cache has the expected structure\n",
    "    expected_keys = set(RUNS.keys())\n",
    "    cached_keys = set(run_data.keys())\n",
    "    \n",
    "    if cached_keys == expected_keys:\n",
    "        print(\"Cache is valid, using cached data.\")\n",
    "    else:\n",
    "        print(f\"Cache keys mismatch. Expected: {expected_keys}, Got: {cached_keys}\")\n",
    "        print(\"Regenerating data...\")\n",
    "        run_data = None\n",
    "else:\n",
    "    print(\"No cache found, generating data...\")\n",
    "    run_data = None\n",
    "\n",
    "if run_data is None:\n",
    "    # Collect data for all runs\n",
    "    run_data = defaultdict(lambda: {\"mutation_ids\": [], \"mutation_prob_mass\": []})\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for experiment in RUNS:\n",
    "        for run_id in RUNS[experiment]:\n",
    "            mutation_ids_list = []\n",
    "            mutation_prob_mass_list = []\n",
    "            for step in STEPS:\n",
    "                mutation_ids, mutation_prob_mass = get_mutation_probs_from_checkpoint(\n",
    "                    os.path.join(NOTEBOOK_PATH, \"ckpts\", 'sequential' if SEQUENTIAL else 'non_sequential', experiment, run_id),\n",
    "                    step, \n",
    "                    include_no_mutation=False\n",
    "                )\n",
    "\n",
    "                mutation_ids_list.append(mutation_ids)\n",
    "                mutation_prob_mass_list.append(mutation_prob_mass)\n",
    "            run_data[experiment][\"mutation_ids\"].append(mutation_ids_list)\n",
    "            run_data[experiment][\"mutation_prob_mass\"].append(mutation_prob_mass_list)\n",
    "    \n",
    "    # Save to cache\n",
    "    print(\"Saving data to cache...\")\n",
    "    with open(cache_file, 'wb') as f:\n",
    "        pickle.dump(dict(run_data), f)\n",
    "    print(\"Data cached successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3294f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stacked histogram plot showing mutation evolution over time\n",
    "import re\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def create_mutation_distribution_plot(experiment, run_data, ax, show_legend=False):\n",
    "    \"\"\"Create a stacked area plot for mutation distribution over time.\"\"\"\n",
    "    # Average probability mass across runs for each step\n",
    "    num_runs = len(run_data[experiment][\"mutation_prob_mass\"])\n",
    "    num_steps = len(STEPS)\n",
    "\n",
    "    # Stack all runs and steps to get consistent mutation ordering\n",
    "    all_mutation_ids = set()\n",
    "    for run_idx in range(num_runs):\n",
    "        for step_mutations in run_data[experiment][\"mutation_ids\"][run_idx]:\n",
    "            all_mutation_ids.update(step_mutations.tolist())\n",
    "    all_mutation_ids = list(all_mutation_ids)\n",
    "\n",
    "    # Convert to sorted list\n",
    "    all_mutation_ids = sorted(all_mutation_ids)\n",
    "\n",
    "    # Create matrix: steps x mutations x num_runs \n",
    "    prob_matrix = np.zeros((num_steps, len(all_mutation_ids), num_runs))\n",
    "    for run_idx in range(num_runs):\n",
    "        for step_idx in range(num_steps):\n",
    "            mutation_ids = run_data[experiment][\"mutation_ids\"][run_idx][step_idx]\n",
    "            mutation_probs = run_data[experiment][\"mutation_prob_mass\"][run_idx][step_idx]\n",
    "            for mut_idx, mut_id in enumerate(mutation_ids):\n",
    "                if mut_id in all_mutation_ids:\n",
    "                    global_mut_idx = all_mutation_ids.index(mut_id)\n",
    "                    prob_matrix[step_idx, global_mut_idx, run_idx] = mutation_probs[mut_idx]\n",
    "\n",
    "    # Average across runs for each step\n",
    "    prob_matrix = np.mean(prob_matrix, axis=2)\n",
    "\n",
    "    # Create mutation labels\n",
    "    include_no_mutation = False\n",
    "    if include_no_mutation:\n",
    "        mutation_labels = [mutation_to_str(mutation_id) if mutation_id != MUTATION_PADDING_ID else 'None' for mutation_id in all_mutation_ids]\n",
    "    else:\n",
    "        mutation_labels = [mutation_to_str(mutation_id) for mutation_id in all_mutation_ids if mutation_id != MUTATION_PADDING_ID]\n",
    "\n",
    "    print(prob_matrix.sum(axis=1))\n",
    "    \n",
    "    # Get colors from crest palette\n",
    "    colors = sns.color_palette(\"crest\", len(mutation_labels))\n",
    "\n",
    "    # Create hatching patterns - alternate between no hatch and various patterns\n",
    "    hatch_patterns = ['', '///', '', '\\\\\\\\\\\\', '', '...', '', '+++', '', 'xxx', '', '|||', '', '---']\n",
    "    hatches = [hatch_patterns[i % len(hatch_patterns)] for i in range(len(mutation_labels))]\n",
    "        \n",
    "    def training_steps_to_env_steps(training_steps):\n",
    "        \"\"\"Convert training steps to environment steps by multiplying by 2^21.\"\"\"\n",
    "        return training_steps * (2**21) // 10**6\n",
    "\n",
    "    # Create stacked area plot with alternating hatching\n",
    "    polys = ax.stackplot(np.array(STEPS) * (2**21) // 10**6, *prob_matrix.T, labels=mutation_labels, colors=colors, alpha=0.8)\n",
    "    # Apply hatching to each polygon\n",
    "    for poly, hatch in zip(polys, hatches):\n",
    "        poly.set_hatch(hatch)\n",
    "\n",
    "    base = base_name(experiment)\n",
    "    pretty_name = base_to_pretty.get(base, experiment)\n",
    "\n",
    "    ax.set_xlim(110 * (2**21) // 10**6, 2000 * (2**21) // 10**6)\n",
    "    ax.set_ylim(0, 1)\n",
    "    #! HACKAGE\n",
    "    ax.set_xticklabels([str(x) for x in [0,1000,2000,3000,4000]])\n",
    "    ax.set_yticklabels([str(f\"{x:0.1f}\") for x in ax.get_yticks()])\n",
    "    ax.set_xlabel('Number of Environment Steps (in millions)', fontsize=BIGGER, labelpad=10, fontdict={'family': 'helvetica'})\n",
    "    if not show_legend:\n",
    "        ax.set_ylabel('Fractional Share', fontsize=BIGGER, labelpad=10, fontdict={'family': 'helvetica'})\n",
    "    ax.set_title(f'{pretty_name}', fontsize=BIGGER)\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=BIGGER)\n",
    "    ax.spines['left'].set_linewidth(2)\n",
    "    ax.spines['bottom'].set_linewidth(2)\n",
    "\n",
    "    # Create legend with reversed order to match stacked regions (bottom to top)\n",
    "    if show_legend:\n",
    "        plt.rcParams.update({\n",
    "            \"text.usetex\": True,\n",
    "        })\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        legend = ax.legend(reversed(handles), reversed(labels), bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='large', title='Edit Type')\n",
    "        legend.get_title().set_fontsize('x-large')\n",
    "\n",
    "# Create subplot with 2 columns\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot both experiments\n",
    "create_mutation_distribution_plot(\"accel_full-i\", run_data, ax1, show_legend=False)\n",
    "create_mutation_distribution_plot(\"accel_scratch-i\", run_data, ax2, show_legend=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'plot_outputs/mutation_distribution_comparison.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e8b481caba72a3da"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hurms_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
